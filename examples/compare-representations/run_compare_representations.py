# -*- coding: utf-8 -*-
"""Hands-on: Day 1 - Comparing representations.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JBW5pQ3-HxilyiJuZMREG8uBzzkQwuPS
"""

#!pip install git+https://github.com/pierluigic/languagechange.git
#!pip install umap-learn
#!pip install plotly

"""# **Load the SemEval 2020 Task 1 EN corpus**



"""

from languagechange.benchmark import SemEval2020Task1

semeval_en = SemEval2020Task1('EN')

corpus1 = semeval_en.corpus1_token
corpus2 = semeval_en.corpus2_token

corpus1 = semeval_en.corpus1_lemma
corpus2 = semeval_en.corpus2_lemma

"""# **Load your corpus**"""

import urllib.request

with open('brown_nolines.txt','w+') as g:
  for line in urllib.request.urlopen('http://www.sls.hawaii.edu/bley-vroman/brown_nolines.txt'):
      g.write(line.decode('utf-8'))


from languagechange.corpora import LinebyLineCorpus

brown_corpus = LinebyLineCorpus('brown_nolines.txt', name='Brown Corpus', is_tokenized=False, is_sentence_tokenized=True)

import urllib.request

with open('brown_nolines.txt','w+') as g:
  for line in urllib.request.urlopen('http://www.sls.hawaii.edu/bley-vroman/brown_nolines.txt'):
      g.write(line.decode('utf-8'))

from languagechange.corpora import LinebyLineCorpus

brown_corpus = LinebyLineCorpus('brown_nolines.txt', name='Brown Corpus', is_tokenized=False, is_sentence_tokenized=True)

"""# **Static Embeddings**"""

from languagechange.models.representation.static import StaticModel, CountModel, PPMI, SVD

# CORPUS 1
count_encoder1 = CountModel(corpus1, window_size=10, savepath='count_matrix1')
count_encoder1.encode()
PPMI_encoder1 = PPMI(count_encoder1, shifting_parameter=5, smoothing_parameter=0.75, savepath='ppmi_matrix1')
PPMI_encoder1.encode()
svd_encoder1 = SVD(PPMI_encoder1, dimensionality=100, gamma=1.0, savepath='svd_count_matrix1')
svd_encoder1.encode()
svd_encoder1.load()

# CORPUS 2
count_encoder2 = CountModel(corpus2, window_size=10, savepath='count_matrix2')
count_encoder2.encode()
PPMI_encoder2 = PPMI(count_encoder2, shifting_parameter=5, smoothing_parameter=0.75, savepath='ppmi_matrix2')
PPMI_encoder2.encode()
svd_encoder2 = SVD(count_encoder2, dimensionality=100, gamma=1.0, savepath='svd_count_matrix2')
svd_encoder2.encode()
svd_encoder2.load()

"""# **Align Static Embeddings**"""

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

plane1, plane2 = np.asarray(svd_encoder1['plane_nn'].todense()), np.asarray(svd_encoder2['plane_nn'].todense())
will1, will2 = np.asarray(svd_encoder1['will'].todense()), np.asarray(svd_encoder2['will'].todense())

print('plane1-plane2 similarity', cosine_similarity(plane1, plane2)[0][0])
print('will1-will2 similarity', cosine_similarity(will1, will2)[0][0])

svd_encoder1.row2word()

from languagechange.models.representation.alignment import OrthogonalProcrustes

alignment = OrthogonalProcrustes('aligned1','aligned2')
alignment.align(svd_encoder1, svd_encoder2)

aligned1 = StaticModel('aligned1')
aligned2 = StaticModel('aligned2')
aligned1.load()
aligned2.load()

from sklearn.metrics.pairwise import cosine_similarity

plane1, plane2 = np.asarray(aligned1['plane_nn'].todense()), np.asarray(aligned2['plane_nn'].todense())
will1, will2 = np.asarray(aligned1['will'].todense()), np.asarray(aligned2['will'].todense())

print('plane1-plane2 similarity', cosine_similarity(plane1, plane2)[0][0])
print('will1-will2 similarity', cosine_similarity(will1, will2)[0][0])

"""# **Project Static Embeddings**"""

#!pip install nltk

import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')
stopwords = set(stopwords.words('english'))

svd_encoder1 = SVD(PPMI_encoder1, dimensionality=100, gamma=1.0, savepath='svd_count_matrix1')
svd_encoder1.encode()
svd_encoder1.load()

import numpy as np
from sklearn import preprocessing
from matplotlib import pyplot as plt

encoder = svd_encoder1

def most_similar_idxs(word_idx, M, k):
    sims = np.dot(M[word_idx],M.T)
    return np.flip(np.argsort(sims))[:k]

M = np.asarray(encoder.matrix().todense())
M = preprocessing.normalize(M)
idx2word = encoder.row2word()
word2idx = {idx2word[i]:i for i in idx2word}

idx_word = word2idx['plane_nn']
idxs = most_similar_idxs(idx_word, M, 1000)

clean_idxs = []
for i in idxs:
  if idx2word[i].lower() not in stopwords and len(idx2word[i])>3:
    clean_idxs.append(i)

idxs = clean_idxs[:40]
vectors = M[idxs]
words = [idx2word[i] for i in idxs]

print(words)

from sklearn.manifold import TSNE
tsne = TSNE(n_components=2)
vectors_2d = tsne.fit_transform(vectors)

from sklearn.cluster import AgglomerativeClustering

clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=0.5, linkage='average')
labels = clustering.fit(vectors)

plt.figure(figsize=(5,5))

import matplotlib.pyplot as plt
plt.scatter(vectors_2d[:,0], vectors_2d[:,1])
for i, word in enumerate(words):
    plt.annotate(word, vectors_2d[i])
plt.show()

"""# **Contextualized Embeddings**"""

# target_words = list(semeval_en.binary_task.keys())
# print([str(t) for t in target_words])
# usages = corpus1.search(target_words)
usages = corpus2.search(['bank'])
import random
random.shuffle(usages['bank'])
usages['bank'] = usages['bank'][:100]

import torch
from languagechange.models.representation.contextualized import BERT

device = 'cuda' if torch.cuda.is_available() else 'cpu'
bert = BERT('bert-base-uncased', device=device)
# vectors_bert = bert.encode(usages['graft_nn'])
vectors_bert = bert.encode(usages['bank'])
print(vectors_bert.shape)

from languagechange.models.representation.contextualized import XL_LEXEME

model = XL_LEXEME(device=device)
# vectors_xl_lexeme = model.encode(usages['graft_nn'])
vectors_xl_lexeme = model.encode(usages['bank'])
print(vectors_xl_lexeme.shape)

"""# **Project Contextualised Embeddings**

"""

import plotly.graph_objects as go
from umap.umap_ import UMAP

def project_embeddings(embeddings, sentences):
    # Reduce dimensionality to 2D for visualization (using UMAP in this case)
    reducer = UMAP(n_components=2, random_state=42)
    embeddings_2d = reducer.fit_transform(embeddings)

    # Extract the 2D x and y coordinates
    x = embeddings_2d[:, 0]
    y = embeddings_2d[:, 1]

    # Create scatter plot with Plotly
    fig = go.Figure()

    fig.add_trace(go.Scatter(x=x, y=y, mode='markers',
                               marker=dict(size=10),
                               text=sentences,  # Sentences for hover
                               hoverinfo="text"))

    # Customize layout
    fig.update_layout(title='Embeddings of sentences projected using UMAP',
                      xaxis_title='Dimension 1',
                      yaxis_title='Dimension 2',
                      hovermode='closest',# Tooltip shows info of the closest point
                      width=600,
                      height=600,
                      autosize=False
                      )
    # Show plot
    fig.show()

# project_embeddings(vectors_bert, usages['graft_nn'])
project_embeddings(vectors_bert, usages['bank'])

# project_embeddings(vectors_xl_lexeme, usages['graft_nn'])
project_embeddings(vectors_xl_lexeme, usages['bank'])

a = usages['bank'][8]

for sent in usages['bank']:
  print(sent.text_)